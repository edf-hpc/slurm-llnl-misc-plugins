#!/bin/bash
#
# This script will kill any user processes on a node when the last
# SLURM job there ends. For example, if a user directly logs into
# an allocated node SLURM will not kill that process without this
# script being executed as an epilog.

logger "EPILOG[INFO][$SLURM_JOB_ID]: Starting clean Epilog for job $SLURM_JOB_ID"

if [ x$SLURM_UID == "x" ] ; then 
	exit 0
fi
if [ x$SLURM_JOB_ID == "x" ] ; then 
        exit 0
fi

# Don't try to kill user root or system daemon jobs
if [ $SLURM_UID -lt 1000 ] ; then
	exit 0
fi

# TMPDIR_BASE is the base path used by private-tmpdir SPANK plugin to create
# job private subdirectories. It needs to match the configured base= for this
# plugin in plugstack.conf. Default value is '/tmp/slurm'.
TMPDIR_BASE=/tmp/slurm

# Look at cpuset cgroup controller sysfs to check if user has other running
# jobs on this node. This technique is used to avoid sending RPC to slurmctld
# (eg. with `squeue`) which could generate noticeable load on slurmctld when
# thousands of jobs end in a short period of time.
#
# Note this technique has 2 requirements in slurm.conf:
#
#   PrologFlags=Contain
#   ProctrackType=proctrack/cgroup
#
# The first parameter is required to ensure the cgroup is created even without
# real step (ex: with processes launched using SSH) at job allocation time.

# Find and list all user processes, tmp (outside of TMPDIR_BASE) and shm files
# to destroy. These lists are computed at the beginning of this script to
# minimize risks of conflicts with jobs starting while this script is runnning.
user_procs=$(pgrep -u "$SLURM_UID")
user_shm_files=$(find /dev/shm -name 'psm_shm.*')
user_tmp_files=$(find /tmp -uid "$SLURM_UID" ! -path "${TMPDIR_BASE}.*")

cgroup_uid_dir="/sys/fs/cgroup/cpuset/slurm_$HOSTNAME/uid_$SLURM_UID"
job_list=$([ -d $cgroup_uid_dir ] && (cd $cgroup_uid_dir; ls -d job_* | awk '/job_[0-9][0-9]*/{sub(/^job_/,""); print}'))

# Clean Job actual job cgroups and processes
# this should correct job zombies bug and clean what
# Slurm doesn't clean correctly (processes for non existant jobs in squeue and cgroups)

# we find all processes for actual job and we kill them
logger "EPILOG[INFO][$SLURM_JOB_ID]: killing processes of job $SLURM_JOB_ID" 
find "$cgroup_uid_dir/job_$SLURM_JOB_ID" -name "tasks" -exec kill {} /dev/null \; 2> /dev/null

# Then we delete cgroups
#
# /!\ Please note this is a work-around for a bug observed on old Linux kernel
#     (3.16 branch) that wrongly return EBUSY to slurm when it tried to rmdir
#     the cgroups. See for reference:
#
#       https://github.com/torvalds/linux/commit/91486f61f486662c27ef86dd910f875832e3a5de
#
#     Fairly recent Linux kernels >= 4.4 are fixed and this work-around will be
#     useless once these old kernels are resorbed.
logger "EPILOG[INFO][$SLURM_JOB_ID]: deleting cgroups of job $SLURM_JOB_ID"

CONTROLLERS="cpuset freezer"

for CONTROLLER in $CONTROLLERS; do
	CONTROLLER_JOB_DIR="/sys/fs/cgroup/$CONTROLLER/slurm_$HOSTNAME/uid_$SLURM_UID/job_$SLURM_JOB_ID"
	if [ ! -d "$CONTROLLER_JOB_DIR" ]; then
		logger "EPILOG[INFO][$SLURM_JOB_ID]: cgroups files not found for $CONTROLLER_JOB_DIR"
	else
		logger "EPILOG[ERROR][$SLURM_JOB_ID]: cgroups files found for $CONTROLLER_JOB_DIR"
		number_step_dirs=$(find $CONTROLLER_JOB_DIR -name "step_*" -type d | wc -l)
		logger "EPILOG[ERROR][$SLURM_JOB_ID]: number of step_* directories is $number_step_dirs"
		# find -delete processes in-depth first
		find $CONTROLLER_JOB_DIR -type d -delete
	fi
done

logger "EPILOG[INFO][$SLURM_JOB_ID]: Deleting tmp files for job $SLURM_JOB_ID"
# Remove private-tmpdir SPANK plugin tmp directory, following guidelines from
# plugin developpers:
# https://github.com/hpc2n/spank-private-tmp/blob/master/README
PRIVATE_TMPDIR="${TMPDIR_BASE}.${SLURM_JOB_ID}.${SLURM_RESTART_COUNT:=0}"
if [ -d "${PRIVATE_TMPDIR}" ]; then
   logger "EPILOG[INFO][$SLURM_JOB_ID]: Remove private tmp directory $PRIVATE_TMPDIR"
   rm -rf "${PRIVATE_TMPDIR}"
fi

for job_id in $job_list; do
	if [ "$job_id" != "$SLURM_JOB_ID" ]; then
		logger "EPILOG[INFO][$SLURM_JOB_ID]: another JOB $job_id is running for user $SLURM_UID ($SLURM_JOB_ID) on $HOSTNAME"
		exit 0
	fi
done

# No other SLURM jobs, purge all remaining processes of this user
logger "EPILOG[INFO][$SLURM_JOB_ID]: no other JOB were found for user $SLURM_UID ($SLURM_JOB_ID) on $HOSTNAME"

# Ensure user cgroup is removed if user has no other running job on this node
for CONTROLLER in $CONTROLLERS; do
	rmdir /sys/fs/cgroup/$CONTROLLER/slurm_$HOSTNAME/uid_$SLURM_UID
done

for pid in $user_procs; 
do 
	kill -9 $pid;
	logger "EPILOG[INFO][$SLURM_JOB_ID]: killed remaining running process ($pid) of user ${SLURM_UID} for JOB ${SLURM_JOB_ID} on ${HOSTNAME}" 
done 

# clean shared memory files generated by QLogic PSM stack
logger "EPILOG[INFO][$SLURM_JOB_ID]: Cleaning shared memory files generated by QLogic PSM stack for job $SLURM_JOB_ID"
for shm_file in $user_shm_files
do
	rm -f $shm_file
done

# Remove user tmp files found outside of private-tmpdir SPANK plugin tmp directories.
logger "EPILOG[INFO][$SLURM_JOB_ID]: Deleting additional tmp files for user $SLURM_UID"
IFS=$'\n'
for tmp_file in $user_tmp_files
do
	logger "EPILOG[INFO][$SLURM_JOB_ID]: Remove user additional tmp file $tmp_file"
	rm -rf $tmp_file
done
IFS=''
# Exit cleanly when finishing
exit 0
