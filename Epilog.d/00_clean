#!/bin/bash
#
# This script will kill any user processes on a node when the last
# SLURM job there ends. For example, if a user directly logs into
# an allocated node SLURM will not kill that process without this
# script being executed as an epilog.

if [ x$SLURM_UID == "x" ] ; then 
	exit 0
fi
if [ x$SLURM_JOB_ID == "x" ] ; then 
        exit 0
fi

# Don't try to kill user root or system daemon jobs
if [ $SLURM_UID -lt 1000 ] ; then
	exit 0
fi

# Look at cpuset cgroup controller sysfs to check if user has other running
# jobs on this node. This technique is used to avoid sending RPC to slurmctld
# (eg. with `squeue`) which could generate noticeable load on slurmctld when
# thousands of jobs end in a short period of time.
#
# Note this technique has 2 requirements in slurm.conf:
#
#   PrologFlags=Contain
#   ProctrackType=proctrack/cgroup
#
# The first parameter is required to ensure the cgroup is created even without
# real step (ex: with processes launched using SSH) at job allocation time.

cgroup_uid_dir="/sys/fs/cgroup/cpuset/slurm_$HOSTNAME/uid_$SLURM_UID"
job_list=$([ -d $cgroup_uid_dir ] && (cd $cgroup_uid_dir; ls -d job_* | awk '/job_[0-9][0-9]*/{sub(/^job_/,""); print}'))
for job_id in $job_list; do
	if [ "$job_id" != "$SLURM_JOB_ID" ]; then
		logger "EPILOG[INFO]: another JOB $job_id is running for user $SLURM_UID ($SLURM_JOB_ID) on $HOSTNAME"
		exit 0
	fi
done

# No other SLURM jobs, purge all remaining processes of this user
pkill -KILL -U $SLURM_UID

# clean shared memory files generated by QLogic PSM stack
find /dev/shm -name 'psm_shm.*' -uid $SLURM_UID -delete

# clean /tmp
find /tmp -uid $SLURM_UID -delete

# Exit cleanly when finishing
exit 0
